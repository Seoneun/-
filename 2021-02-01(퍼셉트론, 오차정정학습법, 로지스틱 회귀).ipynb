{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 퍼셉트론\n",
    " - 뉴런의 출력을 y = f(wx+b) 형태로 나타낸 신경망 모델 (y는 예측값, w는 웨이트(가중치), x는 입력값, b는 바이어스)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오차정정학습법\n",
    " - 위 식의 정확성을 높이려면 벡터인 웨이트(w)와 바이어스(b)의 최적값을 찾아야 합니다. 오차정정학습법이란 w, b의 수정 폭을 일반식으로 정립하여 w, b의 최적값을 찾아내는 방법입니다.\n",
    " - (w의 수정 폭) = (t - y)x\n",
    " - (b의 수정 폭) = t - y\n",
    " - t는 예측값이 아닌 실제값, 예측값과 실제값의 차를 통해 수정폭을 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구현 예시\n",
    " - 뉴런이 발화하지 않는 데이터는 평균값이 0이고 발화하는 데이터는 평균값이 5이며 각각 10개의 데이터가 있다고 가정하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(123) # rng라는 난수 객체 생성, np.random.random은 객체가 아님, 123은 seed\n",
    "\n",
    "d = 2 # 데이터의 차원\n",
    "N = 10 # 각 패턴마다의 데이터 수\n",
    "mean = 5 # 뉴련이 발화하는 데이터의 평균값\n",
    "\n",
    "x1 = rng.randn(N, d) + np.array([0, 0])\n",
    "x2 = rng.randn(N, d) + np.array([mean, mean])\n",
    "\n",
    "x = np.concatenate((x1, x2), axis=0) # 행렬 x1, x2의 가로 합 ex) np.concatenate(([a, b] [b, c]), axis=0) = [a, b, c, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.zeros(d)\n",
    "b = 0\n",
    "# 웨이트와 바이어스 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y(x):\n",
    "    return step(np.dot(w, x) + b)\n",
    "\n",
    "def step(x):\n",
    "    return 1 * (x > 0)\n",
    "\n",
    "# step은 계단함수, 뉴런이 발화하려면 입력값에 대한 wx+b가 임계값을 넘어야 한다. 임계값을 넘었는 지를 판단하기 위해 계단함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t(i):\n",
    "    if i < N:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "# N개의 뉴련이 발화하면 1, 아니면 0을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.73736858 6.49073203] 1\n",
      "[2.43834346 3.16080635] -4\n",
      "[6.13951858 4.54533957] -4\n",
      "[2.44219887 2.99745547] -6\n",
      "[5.97363736 7.30140141] -6\n",
      "[4.34630753 3.46317879] -8\n",
      "[2.14037745 1.2763927 ] -9\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    classified = True\n",
    "    for i in range(N * 2):\n",
    "        delta_w = (t(i) - y(x[i])) * x[i]\n",
    "        delta_b = (t(i) - y(x[i]))\n",
    "        w += delta_w\n",
    "        b += delta_b\n",
    "        classified *= all(delta_w == 0) * (delta_b == 0) # all 은 모든 요소가 참이면 True, 반대인 함수로 any가 있음.\n",
    "    if classified:\n",
    "        break\n",
    "    print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(y([0, 0])) # 발화 x\n",
    "print(y([5, 5])) # 발화 o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로지스틱 회귀\n",
    " - 위에서 보았던 단순 퍼셉트론은 발화한다 안 한다의 이분법적 사고로 0, 1로 구분하였습니다. 하지만 세상 일은 맞다, 아니다로만 판단하기 힘들 때가 있습니다. 예를 들어 회색을 보고 검은색인지 흰색인지 딱 잘라 판단하기 힘든 것처럼 말입니다. 때문에 계단함수(step function)을 대체할 수 있는 활성화 함수(activaition function)이 필요합니다. 이 때 로지스틱 회귀에서 (로지스틱) 시그모이드 함수를 다뤄보겠습니다.\n",
    " - sigmoid function = 1 / (1 + exp(-x))\n",
    " - 0 < sigmoid function < 1 - 확률론적으로 계산 가능\n",
    " - 계단 함수 대신 시그모이드 함수를 사용한 모델을 로지스틱 회귀라 부릅니다.\n",
    " - 활성화 함수란(activation function)란 뉴런의 선형결합 후 비선형변환을 진행해주는 함수를 뜻합니다. ex) ReLu, sigmoid, step, etc\n",
    " - 시그모이드 함수를 사용하는 이유: (시그모이드 함수 미분값) = (시그모이드 함수)(1 - 시그모이드 함수) 꼴이기 떄문\n",
    " - C = 1 일 때, 뉴런이 발화, C = 0 일 때, 뉴런이 발화하지 않는다. - 확률변수라 생각\n",
    " - p(C=1 | x) = sigmoid(wx+b)\n",
    " - p(C=0 | x) = 1 - p(C=1 | x)\n",
    " - y = sigmoid(wx+b)라 생각하면, p(C=t | x) = y^t*(1-y)^(1-t), (t는 0 또는 1)로 식을 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 우도 함수\n",
    " - x의 데이터가 N개 일 때, 각 x값과 쌍을 이루는 각 t에 대하여 L(w, b) = p(C=t1 | x1) * p(C=t2 | x2) * p(C=t3 | x3) * ... * p(C=tN | xN)\n",
    " - L(w, b) = y1^t1*(1-y1)^(1-t1) * y2^t2*(1-y2)^(1-t2) * y3^t3*(1-y3)^(1-t3) * ... * yN^tN*(1-yN)^(1-tN)\n",
    " - 최적화(optimizer)를 하려면 우도 함수의 값을 최소나 최대가 되도록 하는 것입니다. 대체로 함수의 최대화는 부호를 반전시키면 최소화가 되므로 일반적으로 함수를 최적화한다라고 할 때는 함수를 최소로 만드는 파라미터를 구하는 것을 의미합니다. - 미분의 개념\n",
    " - 다만 우도 함수의 꼴이 다항식의 곱의 꼴이므로 미분하는 것이 까다롭습니다. 따라서 우변항과 좌변항의 로그를 취해 곱셈식을 덧셈식으로 치환합니다. E(w, b) = log(L(w, b)) = -sigma(n=1부터n=N까지)(tnlog(yn) + (1-tn)log(1-yn)) - 교차 엔트로피 함수(cross-entropy error function)\n",
    " - 위 함수를 오차 함수 혹은 손실 함수라 부릅니다. ex) MSE(Mean Squared Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사하강법\n",
    " - 교차 엔트로피 오차 함수에서 입력값 w, b에 대해 w, b를 각각 편미분 해서 0이 되는 값을 구해야 합니다. 하지만 이를 식으로 풀어 구하기는 쉽지 않습니다. 따라서 반복학습을 통해 입력값을 순차적으로 갱신해서 구하는 방법을 사용해야 합니다. (경사하강법)\n",
    " - w(k+1번째) = w(k번째) - (학습률)*(E(w,b)를 w로 편미분한 값), b(k+1번째) = b(k번째) - (학습률)*(E(w,b)를 b로 편미분한 값)\n",
    " - 학습률이란 하이퍼 파라미터(사용자가 직접 설정해주는 값)로, 모델이 수렴되는 정도를 나타냅니다. 대체로 0.1이나 0.01을 사용합니다. 학습률이 너무 작으면 학습이 오래걸리고 대역최적해가 아닌 국소최적해를 찾을 수도 있다는 단점이 있고, 학습률이 너무 크면 최저점을 무질서하게 이탈할 가능성(수렴하기 힘들어짐)이 있습니다. \n",
    " - 파라미터(w,b)가 더 이상 갱신되지 않는다면 경사가 0이 됐다는 뜻으로 가장 적합한 해를 구했다는 것을 의미합니다.\n",
    " - (E(w,b)를 w로 편미분한 값) = -sigma(n=1부터 n=N까지)(tn-yn)xn, (E(w,b)를 b로 편미분한 값) = -sigma(n=1부터 n=N까지)(tn-yn)\n",
    " - w(k+1번째) = w(k번째) + (학습률)*(sigma(n=1부터 n=N까지)(tn-yn)xn)), b(k+1번째) = b(k번째) + (학습률)*(sigma(n=1부터 n=N까지)(tn-yn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 확률 경사하강법과 미니배치 경사하강법\n",
    " - 경사하강법을 통해 로지스틱 회귀를 사용할 수 있지만 파라미터를 갱신할 때마다 N개의 모든 데이터의 합을 구해야 한다는 문제가 있습니다. N이 작을 때는 뭐 큰 문제가 아니지만, 만약 N이 엄청 크다면 메모리가 부족하거나 계산 시간이 엄청 오래 걸립니다. 이 문제를 해결하기 위해 확률 경사하강법을 사용합니다.\n",
    " - 확률 경사하강법은 데이터를 하나씩 무작위로 골라서 파라미터를 N번 변경하는 기법입니다.\n",
    " - 즉, w(k+1번째) = w(k번째) + (학습률)*(tn-yn)xn), b(k+1번째) = b(k번째) + (학습률)*(tn-yn)로 생각하여 N번 파라미터를 바꿉니다.\n",
    " - 이 데이터 전체에 대한 반복 횟수를 epoch(에폭)이라 부릅니다. 1epoch만으로 경사가 0으로 수렴하는 경우는 거의 없으므로 epoch횟수를 적절히 늘려 최적인 해를 찾아야 합니다.\n",
    " - 미니배치 경사하강법이란 경사 하강법과 확률 경사하강법의 중간에 존재하는 기법으로 N개의 데이터를 M개씩 쪼개어 학습하는 방법입니다. 보통 M을 50~500정도로 사용하며 메모리의 부족함 없이 선형대수를 연산하여 데이터 연산 속도를 빠르게 할 수 있다는 장점이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구현 예시\n",
    " - 덴서플로와 케라스를 이용하여 간단한 OR게이트 학습을 예로 들어 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.zeros([2,1])) # OR게이트는 입력이 2차원이고 출력이 1차원\n",
    "b = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y(x):\n",
    "    return sigmoid(np.dot(w,x) + b)\n",
    "\n",
    "def sigmoid(x): # Activation function\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "t = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "y = tf.nn.sigmoid(tf.matmul(x, w) + b) # tf.matmul은 np.dot의 tf버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(t * tf.log(y) + (1 - t) * tf.log(1 - y)) # 손실함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "#오차 함수를 각 파라미터로 편미분해 최적화하는 함수입니다. (0.1은 학습률), 직접 수식으로 계산안 해도 함수를 사용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.to_float(tf.greater(y, 0.5)), t)\n",
    "# y >= 0.5가 발화하는 기준입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [1]])\n",
    "# OR게이트 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "# 텐서플로는 데이터를 세션이라는 것 안에서 계산합니다. 세션을 만드는 방법은 위와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(200):\n",
    "    sess.run(train_step, feed_dict={\n",
    "        x: X,\n",
    "        t: Y\n",
    "    })\n",
    "# 경사하강법을 통해 실제로 학습하는 부분입니다. placeholder에 feed_dict을 사용해 x에 X, t에 Y를 대입합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "classified = correct_prediction.eval(session=sess, feed_dict={\n",
    "    x: X,\n",
    "    t: Y\n",
    "})\n",
    "print(classified)\n",
    "# 제대로 학습됐는지 확인하는 부분입니다. eval은 텐서에 담겨있는 결과를 볼 수 있는 기본명령어입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22355038]\n",
      " [0.9142595 ]\n",
      " [0.9142595 ]\n",
      " [0.9974742 ]]\n"
     ]
    }
   ],
   "source": [
    "prob = y.eval(session=sess, feed_dict={\n",
    "    x: X,\n",
    "    t: Y\n",
    "})\n",
    "print(prob)\n",
    "#각 입력에 대한 발화 확률입니다. OR게이트 의도대로 발화할 것을 알 수 있습니다.(확률이 0.5이상일 때만 발화한다고 위에서 가정함.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [[3.6118839]\n",
      " [3.6118839]]\n",
      "b: [-1.2450949]\n"
     ]
    }
   ],
   "source": [
    "print('w:', sess.run(w))\n",
    "print('b:', sess.run(b))\n",
    "# tf.Variable()로 정의한 변수는 .eval()이 아니라 sess.run을 통해서 구해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 케라스로 구현\n",
    " - 텐서플로는 개발자가 직접 수식을 프로그래밍했지만 케라스는 x나 y를 생각할 필요 없이 모델을 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(input_dim=2, units=1), # 입력 2차원, 출력 1차원\n",
    "    Activation('sigmoid') # 활성화 함수는 sigmoid 함수\n",
    "])\n",
    "# model을 미리 Sequential로 만들긴 했지만 .add()함수로 필요한 부분(층)을 추가할 수 있습니다.\n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_dim=2, units=1))\n",
    "# model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1)) # SGD(Stochastic Gradient Descent) = 확률 경사하강법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/200\n",
      "4/4 [==============================] - 0s 96ms/step - loss: 0.5361\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5049\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4807\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4615\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4453\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4318\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4203\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4103\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4014\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3934\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3864\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.3797\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3738\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3682\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3629\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3578\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.3530\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3485\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3442\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3400\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3359\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3320\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3282\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3246\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3210\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3175\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.3141\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3108\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3075\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.3044\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3012\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2981\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2952\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2922\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2894\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2865\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2837\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2810\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2784\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2758\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2732\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2706\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2682\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2657\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2633\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2609\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2586\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2563\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2540\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2518\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2496\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2475\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2454\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2433\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2413\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2393\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2373\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2353\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2334\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2315\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2296\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2278\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2260\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2242\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2225\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2207\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 750us/step - loss: 0.2190\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2173\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2157\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2140\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2124\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2109\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2093\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.2077\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2062\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 750us/step - loss: 0.2047\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2032\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2018\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2003\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1989\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1975\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1961\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1947\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1934\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1921\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1907\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1894\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1882\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1869\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1856\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1844\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1832\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 750us/step - loss: 0.1820\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1808\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1796\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1784\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1773\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1762\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1750\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1739\n",
      "Epoch 101/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1728\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1718\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1707\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 750us/step - loss: 0.1696\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1686\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1675\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1665\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1655\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 750us/step - loss: 0.1645\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1635\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1626\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1616\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1606\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1597\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1588\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1579\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1569\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1560\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1551\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1543\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1534\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1525\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1517\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1508\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1500\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1491\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1483\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1475\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1467\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1459\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1451\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1443\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1436\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1428\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1420\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1413\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1406\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1398\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1391\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1384\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1377\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1370\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1363\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1356\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1349\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1342\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1335\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1329\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1322\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1315\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1309\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1303\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1296\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1290\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1284\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1277\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1271\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 750us/step - loss: 0.1265\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1259\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1253\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1247\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1241\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1236\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1230\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1224\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1218\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1213\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1207\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1202\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1196\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1191\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1185\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1180\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1175\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1170\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1164\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1159\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1154\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1149\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1144\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1139\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1134\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1129\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1124\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1119\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1115\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1110\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1105\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1100\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1096\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1091\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1087\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1082\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1078\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1073\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1069\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1064\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1060\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1056\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.1051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0xff4f8c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=200, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict_classes(X, batch_size=1)\n",
    "prob = model.predict_proba(X, batch_size=1)\n",
    "# 학습 후에 나온 결과 담기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classified:\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "output probability:\n",
      "[[0.21435973]\n",
      " [0.9213187 ]\n",
      " [0.91340435]\n",
      " [0.9977958 ]]\n"
     ]
    }
   ],
   "source": [
    "print('classified:')\n",
    "print(Y == classes)\n",
    "print()\n",
    "print('output probability:')\n",
    "print(prob)\n",
    "# 확률이 위에 꺼랑 조금은 다르지만 거의 유사하게 나온 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
