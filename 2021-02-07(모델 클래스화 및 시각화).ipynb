{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.core import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 784\n",
    "n_hiddens = [200, 200]\n",
    "n_out = 10\n",
    "activation = 'relu'\n",
    "p_keep = 0.5\n",
    "\n",
    "model = Sequential()\n",
    "for i, input_dim in enumerate(([n_in] + n_hiddens)[:-1]):\n",
    "    model.add(Dense(n_hiddens[i], input_dim=input_dim))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(p_keep))\n",
    "    \n",
    "model.add(Dense(n_out))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:85: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22. Please use fetch_openml.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:85: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22. Please use fetch_openml.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = datasets.fetch_mldata('MNIST original', data_home='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(mnist.data)\n",
    "N = 10000\n",
    "indices = np.random.permutation(range(n))[:N]\n",
    "X = mnist.data[indices]\n",
    "y = mnist.target[indices]\n",
    "Y = np.eye(10)[y.astype(int)]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.005), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 0.9836 - accuracy: 0.6407\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 1.0410 - accuracy: 0.6324\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 1.0414 - accuracy: 0.6296\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 1.0541 - accuracy: 0.6201\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1.0205 - accuracy: 0.6346\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 1.0302 - accuracy: 0.6291\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 1.0126 - accuracy: 0.6342\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 1.0369 - accuracy: 0.6286\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 1.0164 - accuracy: 0.6373\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 1.0326 - accuracy: 0.6306\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 1.0154 - accuracy: 0.6324\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 0.9953 - accuracy: 0.6389\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1.0026 - accuracy: 0.6407\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1.0081 - accuracy: 0.6394\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 0.9923 - accuracy: 0.6405\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 72us/step - loss: 0.9831 - accuracy: 0.6380\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 0.9795 - accuracy: 0.6396\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 0.9443 - accuracy: 0.6590\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 0.9292 - accuracy: 0.6670\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 0.9623 - accuracy: 0.6531\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 0.9269 - accuracy: 0.6746\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.9768 - accuracy: 0.6584\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 0.9504 - accuracy: 0.6697\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 3s 345us/step - loss: 0.9694 - accuracy: 0.6685\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 4s 483us/step - loss: 0.9151 - accuracy: 0.6860\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 4s 465us/step - loss: 0.9154 - accuracy: 0.6884\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 4s 531us/step - loss: 0.9236 - accuracy: 0.6845\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 0.9075 - accuracy: 0.6880\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 0.9583 - accuracy: 0.6752\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 0.9164 - accuracy: 0.6920\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 0.9130 - accuracy: 0.6901\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 0.9151 - accuracy: 0.6951\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 0.8960 - accuracy: 0.6969\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 0.8955 - accuracy: 0.7001\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 0.8707 - accuracy: 0.7082\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.8775 - accuracy: 0.6988\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 0.8711 - accuracy: 0.7044\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 0.8663 - accuracy: 0.7016\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 0.8580 - accuracy: 0.7101\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 0.8614 - accuracy: 0.7074\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 0.8634 - accuracy: 0.7079\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.8583 - accuracy: 0.70 - 1s 70us/step - loss: 0.8571 - accuracy: 0.7031\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.8467 - accuracy: 0.7117\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 0.8398 - accuracy: 0.7172\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 0.8446 - accuracy: 0.7120\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 0.8497 - accuracy: 0.7143\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 0.8595 - accuracy: 0.7106\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 0.8490 - accuracy: 0.7117\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 0.8352 - accuracy: 0.7166\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 0.8287 - accuracy: 0.7156\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 0.8413 - accuracy: 0.7095\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 0.8800 - accuracy: 0.7129\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 0.8352 - accuracy: 0.7129\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 0.8250 - accuracy: 0.7180\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 0.8339 - accuracy: 0.7174\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.8396 - accuracy: 0.7116\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.8406 - accuracy: 0.7129\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 0.8177 - accuracy: 0.7199\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 0.8275 - accuracy: 0.7196\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 0.8108 - accuracy: 0.7266\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.7898 - accuracy: 0.7309\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8225 - accuracy: 0.7206\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.7928 - accuracy: 0.7315\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 0.8007 - accuracy: 0.7287\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 0.7965 - accuracy: 0.7286\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 0.8012 - accuracy: 0.7303\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 70us/step - loss: 0.7953 - accuracy: 0.7270\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 0.8031 - accuracy: 0.7241\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 0.7988 - accuracy: 0.7258\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 0.8009 - accuracy: 0.7272\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 0.8050 - accuracy: 0.7303\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7915 - accuracy: 0.7305\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 0.7739 - accuracy: 0.7387\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 0.8005 - accuracy: 0.7299\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 0.7604 - accuracy: 0.7409\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 0.7868 - accuracy: 0.7380\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 74us/step - loss: 0.7880 - accuracy: 0.7349\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7719 - accuracy: 0.7369\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 0.7537 - accuracy: 0.7494\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 0.7628 - accuracy: 0.7408\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 0.7580 - accuracy: 0.7458\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 0.7401 - accuracy: 0.7530\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 0.7663 - accuracy: 0.7442\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 0.7567 - accuracy: 0.7441\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 0.7282 - accuracy: 0.7539\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.7367 - accuracy: 0.7491\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 0.7665 - accuracy: 0.7499\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.7631 - accuracy: 0.7445\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 0.7603 - accuracy: 0.7491\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 0.7537 - accuracy: 0.7473\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 0.7447 - accuracy: 0.7513\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.7304 - accuracy: 0.7558\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 0.7183 - accuracy: 0.7591\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 0.7446 - accuracy: 0.7502\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 0.7373 - accuracy: 0.7538\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 0.7305 - accuracy: 0.7552\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 0.7261 - accuracy: 0.7565\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 0.7288 - accuracy: 0.7544\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.7195 - accuracy: 0.7566\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 0.7123 - accuracy: 0.7576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x16488e48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 69us/step\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5590709671974182, 0.906000018119812]\n"
     ]
    }
   ],
   "source": [
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    def __init__(self, n_in, n_hiddens, n_out):\n",
    "        #초기화 처리\n",
    "        self.n_in = n_in\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.n_out = n_out\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self._x = None\n",
    "        self._t = None\n",
    "        self._keep_prob = None\n",
    "        self._sess = None\n",
    "        self._history = {\n",
    "            'accuracy': [],\n",
    "            'loss': []\n",
    "        }\n",
    "        self.accuracy_temp = None\n",
    "        \n",
    "    def weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def inference(self, x, keep_prob):\n",
    "        # 입력층 - 은닉층, 은닉층 - 은닉층\n",
    "        for i, n_hidden in enumerate(self.n_hiddens):\n",
    "            if i == 0:\n",
    "                input = x\n",
    "                input_dim = self.n_in\n",
    "            else:\n",
    "                input = output\n",
    "                input_dim = self.n_hiddens[i-1]\n",
    "            \n",
    "            self.weights.append(self.weight_variable([input_dim, n_hidden]))\n",
    "            self.biases.append(self.bias_variable([n_hidden]))\n",
    "            \n",
    "            h = tf.nn.relu(tf.matmul(input, self.weights[-1]) + self.biases[-1])\n",
    "            output = tf.nn.dropout(h, keep_prob)\n",
    "            \n",
    "        # 은닉층 - 출력층\n",
    "        self.weights.append(self.weight_variable([self.n_hiddens[-1], self.n_out]))\n",
    "        self.biases.append(self.bias_variable([self.n_out]))\n",
    "        \n",
    "        y = tf.nn.softmax(tf.matmul(output, self.weights[-1]) + self.biases[-1])\n",
    "        return y\n",
    "    \n",
    "    def loss(self, y, t):\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(t*tf.log(y), reduction_indices=[1]))\n",
    "        return cross_entropy\n",
    "    \n",
    "    def training(self, loss):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        return train_step\n",
    "    \n",
    "    def accuracy(self, y, t):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        self.accuracy_temp = accuracy\n",
    "        return accuracy\n",
    "    \n",
    "    def fit(self, X_train, Y_train, epochs=100, batch_size=100, p_keep=0.5, verbose=1):\n",
    "        # 학습 처리\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.n_in])\n",
    "        t = tf.placeholder(tf.float32, shape=[None, self.n_out])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # evaluate()용으로 작성해둔다\n",
    "        self._x = x\n",
    "        self._t = t\n",
    "        self._keep_prob = keep_prob\n",
    "        \n",
    "        y = self.inference(x, keep_prob)\n",
    "        loss = self.loss(y, t)\n",
    "        train_step = self.training(loss)\n",
    "        accuracy = self.accuracy(y, t)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "        \n",
    "        # evaluate()용으로 작성해둔다\n",
    "        self._sess = sess\n",
    "        \n",
    "        N_train = len(X_train)\n",
    "        n_batches = 20#int(N_train)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            X_, Y_ = shuffle(X_train, Y_train)\n",
    "            \n",
    "            for i in range(n_batches):\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "                \n",
    "                sess.run(train_step, feed_dict={\n",
    "                    x: X_[start:end],\n",
    "                    t: Y_[start:end],\n",
    "                    keep_prob: p_keep\n",
    "                })\n",
    "            loss_ = loss.eval(session=sess, feed_dict={\n",
    "                x: X_train,\n",
    "                t: Y_train,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            accuracy_ = accuracy.eval(session=sess, feed_dict={\n",
    "                x: X_train,\n",
    "                t: Y_train,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            # 값을 기록해둔다\n",
    "            self._history['loss'].append(loss_)\n",
    "            self._history['accuracy'].append(accuracy_)\n",
    "            \n",
    "            if verbose:\n",
    "                print('epochs:', epoch, 'loss:', loss_, 'accuracy:', accuracy_)\n",
    "                return self._history\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        # 평가 처리\n",
    "        return self.accuracy_temp.eval(session=self._sess, feed_dict={\n",
    "            self._x: X_test,\n",
    "            self._t: Y_test,\n",
    "            self._keep_prob: 1.0\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0 loss: 2.2929003 accuracy: 0.2755\n",
      "accuracy: 0.2614\n"
     ]
    }
   ],
   "source": [
    "model = DNN(n_in=784, n_hiddens=[200, 200, 200], n_out=10)\n",
    "model.fit(X_train, Y_train, epochs=50, batch_size=200, p_keep=0.5)\n",
    "accuracy = model.evaluate(X_test, Y_test)\n",
    "print('accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 5000\n",
    "N_validation = 4000\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=N_train)\n",
    "\n",
    "# 훈련 데이터를 훈련 데이터와 검증데이터\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, train_size=N_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = len(X[0]) # 784\n",
    "n_hidden = 200\n",
    "n_out = len(Y[0]) # 10\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, n_in])\n",
    "t = tf.placeholder(tf.float32, shape=[None, n_out])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# 입력층 - 은닉층\n",
    "W0 = tf.Variable(tf.truncated_normal([n_in, n_hidden], stddev=0.01))\n",
    "b0 = tf.Variable(tf.zeros([n_hidden]))\n",
    "h0 = tf.nn.relu(tf.matmul(x, W0) + b0)\n",
    "h0_drop = tf.nn.dropout(h0, keep_prob)\n",
    "\n",
    "# 은닉층 - 은닉층\n",
    "W1 = tf.Variable(tf.truncated_normal([n_hidden, n_hidden], stddev=0.01))\n",
    "b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "h1 = tf.nn.relu(tf.matmul(h0_drop, W1) + b1)\n",
    "h1_drop = tf.nn.dropout(h1, keep_prob)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([n_hidden, n_hidden], stddev=0.01))\n",
    "b2 = tf.Variable(tf.zeros([n_hidden]))\n",
    "h2 = tf.nn.relu(tf.matmul(h1_drop, W2) + b2)\n",
    "h2_drop = tf.nn.dropout(h2, keep_prob)\n",
    "\n",
    "# 은닉층 - 출력층\n",
    "W3 = tf.Variable(tf.truncated_normal([n_hidden, n_out], stddev=0.01))\n",
    "b3 = tf.Variable(tf.zeros([n_out]))\n",
    "y = tf.nn.softmax(tf.matmul(h2_drop, W3) + b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(-tf.reduce_sum(t*tf.log(y), reduction_indices=[1]))\n",
    "cross_entropy = -tf.reduce_sum(t * tf.log(y) + (1-t)*tf.log(y-1))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.to_float(tf.greater(y, 0.5)), t)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "epochs = 100 \n",
    "n_batches = 10\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    X_, Y_ = shuffle(X_train, Y_train)\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        sess.run(train_step, feed_dict={\n",
    "            x: X_[start:end],\n",
    "            t: Y_[start:end],\n",
    "            keep_prob: p_keep\n",
    "        })\n",
    "        \n",
    "    val_loss = loss.eval(session=sess, feed_dict={\n",
    "        x: X_validation,\n",
    "        t: Y_validation,\n",
    "        keep_prob: 1.0\n",
    "    })\n",
    "    val_acc = accuracy.eval(session=sess, feed_dict={\n",
    "        x: X_validation,\n",
    "        t: Y_validation,\n",
    "        keep_prob: 1.0\n",
    "    })\n",
    "    \n",
    "    # 검증 데이터에 대한 학습 진행 상황을 기록\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (100,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-3a91705150fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'black'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epochs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2793\u001b[0m     return gca().plot(\n\u001b[0;32m   2794\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2795\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1664\u001b[0m         \"\"\"\n\u001b[0;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1666\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1667\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[1;32m--> 270\u001b[1;33m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "plt.rc('font', family='serif')\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(range(epochs), history['val_acc'], label='acc', color='black')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('validation loss')\n",
    "\n",
    "plt.show()\n",
    "'''\n",
    "# 이부분은 나중에 고쳐보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
